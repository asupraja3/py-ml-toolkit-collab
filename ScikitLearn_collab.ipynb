{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3lHm75NvenMLa4NCU3iub",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asupraja3/py-ml-toolkit-collab/blob/main/ScikitLearn_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤– What is Scikit-learn?\n",
        "\n",
        "**Scikit-learn** is a free, open-source machine learning library for Python that provides simple and efficient tools for data mining and data analysis. It's built on top of **NumPy**, **SciPy**, and **matplotlib**, and is one of the most widely used ML libraries in the Python ecosystem.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”§ Key Features\n",
        "\n",
        "- **Classification** â€“ Identifying labels or categories  \n",
        "  *Example: Email spam detection, disease diagnosis*\n",
        "\n",
        "- **Regression** â€“ Predicting continuous values  \n",
        "  *Example: Predicting house prices, stock prices*\n",
        "\n",
        "- **Clustering** â€“ Grouping similar data points without labels  \n",
        "  *Example: Customer segmentation using KMeans*\n",
        "\n",
        "- **Dimensionality Reduction** â€“ Reducing the number of features  \n",
        "  *Example: PCA (Principal Component Analysis)*\n",
        "\n",
        "- **Model Selection** â€“ Tools for choosing and validating models  \n",
        "  *Example: Cross-validation, GridSearchCV*\n",
        "\n",
        "- **Preprocessing** â€“ Scaling, transforming, and encoding data  \n",
        "  *Example: StandardScaler, LabelEncoder, OneHotEncoder*\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“š Use Cases\n",
        "\n",
        "- Predicting housing prices using regression\n",
        "- Detecting fraudulent transactions using classification\n",
        "- Segmenting customers with clustering\n",
        "- Compressing data features with PCA\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ’¡ Why Use Scikit-learn?\n",
        "\n",
        "- âœ… Beginner-friendly API\n",
        "- âœ… Consistent model syntax (.fit, .predict, .score)\n",
        "- âœ… Excellent documentation & community support\n",
        "- âœ… Integrates well with pandas, NumPy, and Jupyter\n",
        "- âœ… Great for fast prototyping and academic research\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Eh1EPvkWy2uU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Imports and Setup**\n",
        "Interview Context: AI Engineers need to know the standard stack (numpy, pandas, sklearn)."
      ],
      "metadata": {
        "id": "vpgd4Q-TfCTc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FSZwMfl6f8al",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c42b36a-bf55-4cb6-8826-ca2244a3f513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries loaded successfully. Ready for Financial Modeling.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris, make_regression, make_classification\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "print(\"Libraries loaded successfully. Ready for Financial Modeling.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2: Dataset Loading (The Basics + Financial Context)**\n",
        "Topic: load_iris, train_test_split\n",
        "\n",
        "Interview Context: You were asked about load_iris (a classic benchmark), but in a Citadel interview, you'd deal with tabular financial data."
      ],
      "metadata": {
        "id": "ViVQiQeYhylM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. The requested 'load_iris' practice (Standard Interview Check)\n",
        "iris = load_iris()\n",
        "X_iris, y_iris = iris.data, iris.target\n",
        "print(f\"Iris Data Shape: {X_iris.shape}\")\n",
        "\n",
        "# 2. REAL SCENARIO: Credit Application Data (Citadel Context)\n",
        "# Features: [Credit Score, Annual Income, Debt-to-Income Ratio, Years employed]\n",
        "# Target: 1 (Default/Reject), 0 (Repay/Approve)\n",
        "X_fin, y_fin = make_classification(n_samples=1000, n_features=4, random_state=42)\n",
        "\n",
        "# Splitting the data (Crucial to prevent data leakage)\n",
        "# Test size 0.2 means 20% of data is hidden for final validation\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_fin, y_fin, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Financial Data split complete.\")\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Testing samples: {X_test.shape[0]}\")"
      ],
      "metadata": {
        "id": "R6n3ZWVwiK-O",
        "outputId": "805a6351-5519-4252-af2b-1707cdb36c01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris Data Shape: (150, 4)\n",
            "Financial Data split complete.\n",
            "Training samples: 800\n",
            "Testing samples: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3: Preprocessing (Scaling & Encoding)**\n",
        "Topic: Scaling (StandardScaler), Encoding\n",
        "\n",
        "Interview Context: Algorithms like KNN and SVM calculate \"distance.\" If Income is 100,000 and Credit Score is 700, the Income variable will dominate the model purely because the number is bigger. Scaling fixes this."
      ],
      "metadata": {
        "id": "0LJwFBWrklDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Scaling (Standardization)\n",
        "# We fit the scaler ONLY on training data to avoid looking into the future (Data Leakage)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Data Scaled (Mean approx 0, Variance approx 1)\")\n",
        "print(f\"Sample scaled feature row: {X_train_scaled[0]}\")\n",
        "\n",
        "# 2. Encoding (For categorical data)\n",
        "# Scenario: Converting 'Sector' (Tech, Energy, Finance) into numbers\n",
        "sectors = pd.DataFrame({'sector': ['Tech', 'Energy', 'Finance', 'Tech']})\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "sectors_encoded = encoder.fit_transform(sectors)\n",
        "\n",
        "print(\"\\nOne-Hot Encoded Sectors:\")\n",
        "print(sectors_encoded)"
      ],
      "metadata": {
        "id": "zV-chy0ukolY",
        "outputId": "f9e13cc3-d743-4f6c-9496-2c2c7f4b0f26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Scaled (Mean approx 0, Variance approx 1)\n",
            "Sample scaled feature row: [-0.48469871  0.40799652 -0.15113754  0.2878111 ]\n",
            "\n",
            "One-Hot Encoded Sectors:\n",
            "[[0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4: Regression Models (CDS Pricing)**\n",
        "Topic: Linear, Ridge, Lasso\n",
        "\n",
        "Interview Context: Ridge and Lasso are \"Regularization\" techniques. They prevent overfitting."
      ],
      "metadata": {
        "id": "umbuoABbk0xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating continuous data for Regression (CDS Price Prediction)\n",
        "X_reg, y_reg = make_regression(n_samples=500, n_features=10, noise=0.1, random_state=42)\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2)\n",
        "\n",
        "# 1. Linear Regression (Base model)\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train_reg, y_train_reg)\n",
        "print(f\"Linear Regression Score: {lr.score(X_test_reg, y_test_reg):.4f}\")\n",
        "\n",
        "# 2. Ridge (L2 Regularization - handles multicollinearity in financial markers)\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X_train_reg, y_train_reg)\n",
        "print(f\"Ridge Regression Score: {ridge.score(X_test_reg, y_test_reg):.4f}\")\n",
        "\n",
        "# 3. Lasso (L1 Regularization - Feature selection)\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X_train_reg, y_train_reg)\n",
        "print(f\"Lasso Regression Score: {lasso.score(X_test_reg, y_test_reg):.4f}\")\n",
        "print(\"Note: Lasso is useful to identify which economic indicators actually matter.\")"
      ],
      "metadata": {
        "id": "fqBWze_jlHbz",
        "outputId": "c6d1f366-9d7f-4019-b89c-f575c0cc59eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression Score: 1.0000\n",
            "Ridge Regression Score: 1.0000\n",
            "Lasso Regression Score: 1.0000\n",
            "Note: Lasso is useful to identify which economic indicators actually matter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5: Classification Models (Credit Approval)**\n",
        "Topic: Logistic, KNN, SVM, Decision Tree\n",
        "\n",
        "Scenario: Classifying a trade or loan application as \"Safe\" (0) or \"Risky\" (1)."
      ],
      "metadata": {
        "id": "JqoxPFeHlUFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using our X_train_scaled and y_train from Cell 2\n",
        "\n",
        "# 1. Logistic Regression (Simple, interpretable probabilities)\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 2. KNN (Finds similar historical loans)\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 3. SVM (Finds the best boundary/margin between Safe and Risky)\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 4. Decision Tree (Rule-based: If Score > 700 AND Income > 50k...)\n",
        "dt = DecisionTreeClassifier(max_depth=5)\n",
        "dt.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"All classification models trained successfully.\")"
      ],
      "metadata": {
        "id": "MDt8VKoelYnU",
        "outputId": "6b3f2241-06b7-46e4-a90e-bd4e54df9ac7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All classification models trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6: Model Evaluation**\n",
        "Topic: Accuracy, Confusion Matrix, Classification Report\n",
        "\n",
        "Interview Context: Accuracy is dangerous in finance. If 99% of loans are safe, a model that simply guesses \"Safe\" every time has 99% accuracy but misses every fraud case. We need Precision and Recall."
      ],
      "metadata": {
        "id": "e6CfSzwdlr04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions using SVM\n",
        "y_pred = svm.predict(X_test_scaled)\n",
        "\n",
        "# 1. Accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.2f}\")\n",
        "\n",
        "# 2. Confusion Matrix\n",
        "# [[True Neg (Correct Reject), False Pos (Wrongly Approved)],\n",
        "#  [False Neg (Wrongly Rejected), True Pos (Correct Approve)]]\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# 3. Classification Report (Precision, Recall, F1-Score)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "yUA7islEmDEL",
        "outputId": "f40db4a8-a06e-4175-8ea5-55243d86068d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.88\n",
            "\n",
            "Confusion Matrix:\n",
            "[[92  9]\n",
            " [15 84]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.91      0.88       101\n",
            "           1       0.90      0.85      0.88        99\n",
            "\n",
            "    accuracy                           0.88       200\n",
            "   macro avg       0.88      0.88      0.88       200\n",
            "weighted avg       0.88      0.88      0.88       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7: Cross-Validation & GridSearchCV**\n",
        "Topic: Hyperparameter Tuning\n",
        "\n",
        "Interview Context: How do you know k=5 is best for KNN? Or C=1.0 is best for SVM? You don't. You search for it."
      ],
      "metadata": {
        "id": "SDtG3wjymNsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Cross Validation\n",
        "# splits data into 5 parts, trains on 4, tests on 1, rotates 5 times.\n",
        "scores = cross_val_score(svm, X_train_scaled, y_train, cv=5)\n",
        "print(f\"Cross-Validation Scores: {scores}\")\n",
        "print(f\"Average CV Score: {scores.mean():.2f}\")\n",
        "\n",
        "# 2. GridSearchCV (Automated tuning)\n",
        "# We want to find the best 'C' (regularization) and 'kernel' for SVM\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(SVC(), param_grid, cv=3)\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(f\"\\nBest Parameters found: {grid.best_params_}\")\n",
        "print(f\"Best Estimator Accuracy: {grid.best_score_:.2f}\")"
      ],
      "metadata": {
        "id": "RpbdPo_WmR6k",
        "outputId": "971951ca-17c0-4378-b113-032d6dd0829b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Scores: [0.8375  0.89375 0.85625 0.81875 0.9    ]\n",
            "Average CV Score: 0.86\n",
            "\n",
            "Best Parameters found: {'C': 10, 'kernel': 'rbf'}\n",
            "Best Estimator Accuracy: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8: Pipelines (The Production Standard)**\n",
        "Topic: Pipeline\n",
        "\n",
        "Interview Context: In production (Citadel systems), you never manually scale data then pass it to a model. You wrap them together. This ensures that when new real-time data comes in, it is automatically scaled using the exact same logic as training."
      ],
      "metadata": {
        "id": "0JXKNGkwmXTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the pipeline steps:\n",
        "# Step 1: Scale the data\n",
        "# Step 2: Apply the Classifier\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Fit the WHOLE pipeline on raw data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict on raw test data (Pipeline handles scaling internally)\n",
        "pipe_score = pipeline.score(X_test, y_test)\n",
        "\n",
        "print(f\"Pipeline Accuracy: {pipe_score:.2f}\")\n",
        "print(\"System ready for deployment.\")"
      ],
      "metadata": {
        "id": "UzwaUGwnmfhS",
        "outputId": "6607f4c8-ada5-448d-a71c-915be01c1ab6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline Accuracy: 0.89\n",
            "System ready for deployment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1: Pipeline and ColumnTransformer**\n",
        "Topic: Handling mixed data types (Numbers + Categories) simultaneously.\n",
        "\n",
        "Interview Perspective: Real-world financial data is messy. You have \"Trade Amount\" (Number) and \"Exchange ID\" (Category). You cannot treat them the same. ColumnTransformer allows you to apply different preprocessing to different columns in parallel."
      ],
      "metadata": {
        "id": "fVjjAyP8nOn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Sample Financial Data: [Trade Amount, Risk Score, Exchange ID]\n",
        "data = pd.DataFrame({\n",
        "    'trade_amount': [10000, 50000, 2000, 15000],\n",
        "    'risk_score': [0.1, 0.8, 0.2, 0.4],\n",
        "    'exchange': ['NYSE', 'NASDAQ', 'NYSE', 'LSE']\n",
        "})\n",
        "\n",
        "# Define transformers\n",
        "# 1. Numeric pipeline: Fill missing values -> Scale\n",
        "num_trans = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# 2. Categorical pipeline: Fill missing -> OneHotEncode\n",
        "cat_trans = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder())\n",
        "])\n",
        "\n",
        "# Combine them using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', num_trans, ['trade_amount', 'risk_score']),\n",
        "        ('cat', cat_trans, ['exchange'])\n",
        "    ])\n",
        "\n",
        "# Apply transformations\n",
        "processed_data = preprocessor.fit_transform(data)\n",
        "print(\"Processed Data Shape (Rows, Cols):\", processed_data.shape)\n",
        "print(\"Data is now ready for the model.\")"
      ],
      "metadata": {
        "id": "RdD3OQlKnQsp",
        "outputId": "25a6f7e6-dbc2-4f98-8247-6f5e335bede2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Data Shape (Rows, Cols): (4, 5)\n",
            "Data is now ready for the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2: Preprocessing Module (Scalers & Encoders)**\n",
        "Topic: StandardScaler, MinMaxScaler, OneHotEncoder.\n",
        "\n",
        "Interview Perspective:\n",
        "\n",
        "StandardScaler: Best for algorithms assuming normal distribution (Regression, Logistic).\n",
        "\n",
        "MinMaxScaler: Best for neural networks or bounded ranges (0 to 1).\n",
        "\n",
        "OneHot: Essential for converting text labels to binary vectors."
      ],
      "metadata": {
        "id": "AF4eNSAynWOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Feature: [Asset Price, Daily Volume]\n",
        "market_data = [[150.0, 2000000], [155.0, 2500000], [148.0, 1800000]]\n",
        "\n",
        "# 1. StandardScaler (Mean=0, Variance=1)\n",
        "# Used when data has outliers or normal distribution assumption\n",
        "std_scaler = StandardScaler()\n",
        "print(\"Standard Scaled:\\n\", std_scaler.fit_transform(market_data))\n",
        "\n",
        "# 2. MinMaxScaler (Scales to range [0, 1])\n",
        "# Used for Neural Networks or image data\n",
        "min_max = MinMaxScaler()\n",
        "print(\"\\nMinMax Scaled:\\n\", min_max.fit_transform(market_data))\n",
        "\n",
        "# 3. OneHotEncoder (Categorical -> Binary)\n",
        "# Used for 'Buy', 'Sell', 'Hold' signals\n",
        "signals = [['Buy'], ['Sell'], ['Hold'], ['Buy']]\n",
        "enc = OneHotEncoder(sparse_output=False)\n",
        "print(\"\\nOneHot Encoded Signals:\\n\", enc.fit_transform(signals))"
      ],
      "metadata": {
        "id": "wik4mADfnipD",
        "outputId": "770e08da-7ecc-47ae-ee9d-5a60ee6e4c50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard Scaled:\n",
            " [[-0.33968311 -0.33968311]\n",
            " [ 1.35873244  1.35873244]\n",
            " [-1.01904933 -1.01904933]]\n",
            "\n",
            "MinMax Scaled:\n",
            " [[0.28571429 0.28571429]\n",
            " [1.         1.        ]\n",
            " [0.         0.        ]]\n",
            "\n",
            "OneHot Encoded Signals:\n",
            " [[1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3: Model Training (.fit, .predict, .predict_proba)**\n",
        "Topic: The core training API.\n",
        "\n",
        "Interview Perspective: predict() gives you a hard class (0 or 1). predict_proba() gives you the confidence. In finance, we care about confidence. If the model says \"Fraud\" with 51% confidence, we might ignore it. If 99%, we block the card."
      ],
      "metadata": {
        "id": "I5GD61jZnugW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Synthetic Data: [Debt Ratio, Credit Score] -> [0: No Default, 1: Default]\n",
        "X_train = [[0.1, 800], [0.8, 500], [0.2, 750], [0.9, 450]]\n",
        "y_train = [0, 1, 0, 1]\n",
        "X_new_application = [[0.85, 480]] # A risky application\n",
        "\n",
        "# 1. .fit() -> Trains the model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 2. .predict() -> Hard classification\n",
        "prediction = model.predict(X_new_application)\n",
        "print(f\"Hard Prediction (0=Safe, 1=Default): {prediction[0]}\")\n",
        "\n",
        "# 3. .predict_proba() -> Probability of each class\n",
        "# Output format: [Prob of Class 0, Prob of Class 1]\n",
        "probs = model.predict_proba(X_new_application)\n",
        "risk_percentage = probs[0][1] * 100\n",
        "print(f\"Risk Probability: {risk_percentage:.2f}%\")"
      ],
      "metadata": {
        "id": "JhFyzFVEn6u5",
        "outputId": "eb7f4467-2b3f-4b42-d971-1ab8ae69162a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hard Prediction (0=Safe, 1=Default): 1\n",
            "Risk Probability: 99.99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4: Model Persistence (Saving/Loading)**\n",
        "Topic: joblib vs pickle.\n",
        "\n",
        "Interview Perspective: Training takes hours; prediction takes milliseconds. You train once, save the file, and load it into the production server. joblib is preferred in Scikit-Learn because it handles large numpy arrays more efficiently than Python's built-in pickle."
      ],
      "metadata": {
        "id": "i7hOyc9joYBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# 1. Save the model to disk (simulated)\n",
        "# In real life, this creates a file like 'credit_model.pkl'\n",
        "joblib.dump(model, 'credit_risk_model.pkl')\n",
        "print(\"Model saved to disk.\")\n",
        "\n",
        "# ... System Restart / Transfer to Production Server ...\n",
        "\n",
        "# 2. Load the model from disk\n",
        "loaded_model = joblib.load('credit_risk_model.pkl')\n",
        "\n",
        "# Verify it still works\n",
        "check = loaded_model.predict([[0.1, 800]])\n",
        "print(f\"Loaded Model Prediction check: {check[0]}\")"
      ],
      "metadata": {
        "id": "HLhZk3hioa7M",
        "outputId": "40fe1dc4-bfa9-42b4-ebe5-3a05f0b38a18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to disk.\n",
            "Loaded Model Prediction check: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5: Working with Sparse Matrices**\n",
        "Topic: Handling data that is mostly zeros.\n",
        "\n",
        "Interview Perspective: In NLP (processing news headlines for sentiment) or when OneHotEncoding thousands of stock tickers, your matrix is 99% zeros. Storing every zero wastes RAM. Sparse matrices only store the values and their coordinates.\n",
        "\n",
        "Scenario: Analyzing Bloomberg News headlines (Bag of Words)."
      ],
      "metadata": {
        "id": "O8Rrela5oiYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "import numpy as np\n",
        "\n",
        "# Create a dense matrix (Standard format)\n",
        "# Imagine this is word counts for 3 documents\n",
        "dense_matrix = np.array([\n",
        "    [0, 0, 1, 0],\n",
        "    [0, 2, 0, 0],\n",
        "    [0, 0, 0, 0]\n",
        "])\n",
        "\n",
        "print(f\"Dense size in bytes: {dense_matrix.nbytes}\")\n",
        "\n",
        "# Convert to Compressed Sparse Row (CSR) format\n",
        "# Scikit-learn models accept this format automatically\n",
        "sparse_matrix = sparse.csr_matrix(dense_matrix)\n",
        "\n",
        "print(f\"Sparse (CSR) matrix:\\n{sparse_matrix}\")\n",
        "print(\"Note: Only non-zero elements are stored to save memory.\")"
      ],
      "metadata": {
        "id": "eOYrVkp9oxGZ",
        "outputId": "cd746bb1-9fad-405e-a58b-0c8ed90c3d80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense size in bytes: 96\n",
            "Sparse (CSR) matrix:\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 2 stored elements and shape (3, 4)>\n",
            "  Coords\tValues\n",
            "  (0, 2)\t1\n",
            "  (1, 1)\t2\n",
            "Note: Only non-zero elements are stored to save memory.\n"
          ]
        }
      ]
    }
  ]
}